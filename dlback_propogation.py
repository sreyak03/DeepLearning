# -*- coding: utf-8 -*-
"""DLback propogation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jIouoAWW6YTY-UVKbj0UoSSJCt3453pG
"""

import numpy as np
from sklearn.datasets import make_moons
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split

class MLP:
  def __init__(self,input_size,hidden_size,output_size,learning_rate=.01):
    self.input_size=input_size
    self.hidden_size=hidden_size
    self.output_size=output_size
    self.learning_rate=learning_rate

#Intializes weights and biases

    self.W1=np.random.randn(self.input_size,self.hidden_size)
    self.b1=np.zeros((1,self.hidden_size))
    self.W2=np.random.randn(self.hidden_size,self.output_size)
    self.b2=np.zeros((1,self.output_size))


    def sigmoid(self,x):
      return 1/(1+np.exp(-x))


    def sigmoid_derivative(self,x):
      return x*(1-x)

    def forward(self,X):
      self.z1=np.dot(X,self.W1)+self.b1
      self.a1=self.sigmoid(self.z1)
      self.z2=np.dot(self.a1,self.W2)+self.b2
      self.a2=self.sigmoid(self.z2)
      return self.a2

    def backward(self,X,y):
      m=y.shape[0]

      #compute error

      error=self.a2-y
      d_output =error*self.sigmoid_derivative(self.a2)


      #compute gradient for hidden layer
      error_hidden=np.dot(d_output,self.W2.T)
      d_hidden=error_hidden*self.sigmoid_derivative(self.a1)


      #update weights and biases
      self.W2-=self.learning_rate*np.dot(self.a1.T,d_output)/ m
      self.b2-=self.learning_rate*np.sum(d_output,axis=0,keepdims=True)/ m

      self.W1-=self.learning_rate*np.dot(X.T,d_hidden)/m
      self.b1-=self.learning_rate*np.sum(d_hidden,axis=0,keepdims=True)/m


    def train(self,X,y,epochs=10000):
      for epoch in range(epochs):
        self.forward(X)
        self.backward(X,y)

        if epoch % 1000==0:
          loss=np.mean((self.a2-y)**2)
          print("Epoch",epoch,"loss",loss)

    def predict(self,X):
      return self.forward(X)

#Generate datset

X,y=make_moons(n_samples=500,noise=0.2,random_state=42)
y=y.reshape(-1,1)  #reshape for compatibility


#split dataset
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)

#train MLP

mlp=MLP(input_size=2,hidden_size=4,output_size=1,learning_rate=0.1)
mlp.train(X_train,y_train,epochs=10000)

prediction=mlp.predict(X_test)
print("Prediction",prediction)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons

# Generate the dataset
X, y = make_moons(n_samples=500, noise=0.2, random_state=42)

# Plot the dataset
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')
plt.title("Two Interleaving Moons Dataset")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()